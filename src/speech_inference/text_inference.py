
import streamlit as st
from src import global_configs as cf
from tools.models import facebook_bart, microsoft_phi


@st.cache_data(ttl=cf.STREAMLIT_CONFIG["Streamlit_Application_Configurations"]["Object_TTL"])
def bart_inference(text: str, model_ident: str) -> str:
    """
    Performs text summarization using the BART (Bidirectional and Auto-Regressive Transformer)
    model. The function fetches the model configuration and initializes the BART model with
    proper settings. The input text is processed to generate a summary using the specified
    model.

    Args:
        text (str): The input text to be summarized.
        model_ident (str): Identifier for the BART model configuration to be used.

    Returns:
        str: The summarized text generated by the BART model.
    """

    # Get BART model configurations
    model_configs = cf.MODELS_CONFIG[model_ident]

    # Create an instance of BART model
    model = facebook_bart.FacebookBart(
        model_name=model_configs["Model_Name"],
        device=cf.DEVICE,
        task=model_configs["Model_Task"],
        token_required=model_configs["Hugging_Face_Token"],
        token=None
    )
    summary_output = model.inference(
        input_text=text,
        min_length=model_configs["Minimum_Length"],
        max_length=model_configs["Maximum_Length"]
    )

    return summary_output


@st.cache_data(ttl=cf.STREAMLIT_CONFIG["Streamlit_Application_Configurations"]["Object_TTL"])
def phi4_inference(text: str, model_ident: str, system_prompt: str, user_prompt: str) -> str:
    """
    Runs inference using the Phi4 model based on provided text input, model identifier,
    and specific prompt files for system and user interactions. The function reads
    predefined prompts, configures the model settings, and returns a summarization
    generated by the Phi4 model.

    Args:
        text: The user-provided input text for which the model needs to perform the inference.
        model_ident: Identifier for the model, used to fetch relevant configurations.
        system_prompt: Path to the file containing the system prompt, relative to a predefined
            prompts directory.
        user_prompt: Path to the file containing the user prompt, relative to a predefined
            prompts directory.

    Returns:
        str: The inference output (e.g., summary) generated by the Phi4 model.
    """

    # Get configurations for Phi4 and prompts
    model_configs = cf.MODELS_CONFIG[model_ident]
    system_prompt = cf.PROMPTS_PATH.joinpath(system_prompt).resolve()
    user_prompt = cf.PROMPTS_PATH.joinpath(user_prompt).resolve()

    # Read in system prompt and user prompt
    with open(system_prompt, "r", encoding="utf-8") as f:
        system_prompt_text = f.read()
    with open(user_prompt, "r", encoding="utf-8") as f:
        user_prompt_text = f.read()

    user_prompt_text = f"{user_prompt_text}\n{text}"

    # Instantiate Phi4 model
    model = microsoft_phi.Phi4Instruct(
        model_name=model_configs["Model_Name"],
        model_task=model_configs["Model_Task"],
        token_required=model_configs["Hugging_Face_Token"],
        token=None
    )

    # Generate summary
    summary_output = model.inference(
        system_prompt=system_prompt_text,
        user_prompt=user_prompt_text,
        max_new_tokens=model_configs["Maximum_New_Token"],
        temperature=model_configs["Temperature"],
        top_p=model_configs["Top_P"]
    )

    return summary_output
